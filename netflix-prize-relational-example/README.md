**CMSI 486** Introduction to Database Systems, Fall 2020

# Netflix Prize Relational Database Mini-Stack Example
This folder and README contains sample commands and code that correspond to what is being requested for the [Relational Database Mini-Stack assignment](../README.md). To save repository space, the data files themselves are _not_ included here: before trying out these commands and programs, please [download the files](https://www.kaggle.com/netflix-inc/netflix-prize-data) into this folder first.

To avoid accidental committing of these files, a _.gitignore_ file has been placed here as well. We are definitely not taking an extra two gigabytes of repository space lightly!

## The Greater Schema of Things
The first step in implementing a relational database is to determine its _schema_ (or _logical schema_, if taken in the full context of how these systems are structured). This is done by studying the dataset first in order to get to know its _canonical schema_‚Äîi.e., its overall structure independent of the database that‚Äôs implementing it. Then, with that idea in your head, the structure then needs to be rendered in terms of tables, columns, and keys, as initially described in the [Super Basic SQL](https://dondi.lmu.build/share/db/super-basic-sql.pdf) document.

After studying the Netflix Prize data, this leads to a relational database diagram ([schema.pdf](./schema.pdf)) and corresponding SQL statements that implement this diagram in PostgreSQL ([schema.sql](./schema.sql)). You might recall that [DB Fiddle](https://www.db-fiddle.com) refers to this as the ‚ÄúSchema SQL.‚Äù More formally, this is also known as the _DDL_ (database definition language) portion of SQL‚Äîthe subset of SQL whose role is to describe the database that we‚Äôre building.

The database diagram for this assignment should follow the notation previously given in the [Super Basic Database Diagramming](https://dondi.lmu.build/share/db/super-basic-database-diagramming.pdf) document.

### Differences from the DB Fiddle Schema
Speaking of that document, you‚Äôll notice that the [schema.pdf](./schema.pdf) included here is similar to it but not identical. Mainly, our schema no longer has a _viewer_ table. Ideally, we would, but because we will now be working with the dataset as given, we can‚Äôt really justify having a _viewer_ table because there is no further information provided about the viewer. So we will just take the viewer IDs as values in their own right which don‚Äôt reference any other table.

For [schema.sql](./schema.sql) there is also something new that wasn‚Äôt seen in the corresponding [Netflix DB Fiddle example](https://www.db-fiddle.com/f/o2ohcGVAgHZQg4teg1s9jW/6): the `id` primary key of the _movie_ table has the additional qualifier `GENERATED BY DEFAULT AS IDENTITY`. [`GENERATED BY DEFAULT AS IDENTITY` (search for ‚Äúas identity‚Äù at this link)](https://www.postgresql.org/docs/current/sql-createtable.html) specifies that a column will auto-generate a new value based on an internally-managed sequence (you can type `\ds` into _psql_ to see that sequence if you‚Äôre interested). We‚Äôll use it here so that when new movies are added to the database, we only need to specify `year` and `title`‚Äîthey will automatically get a valid `id`.

## Set the Table(s)
Once your database has been planned out, it‚Äôs time to actually set it up in a running database server. Assuming that you have gotten a [PostgreSQL server up and running](http://dondi.lmu.build/share/db/postgresql-setup-day.pdf), you can pretty much feed your schema file to PostgreSQL via _psql_:

    psql postgres://localhost/postgres -f schema.sql

The example above uses the `-f` option of _psql_ to send a file containing SQL statements directly into the database.

Of course, you can also just run _psql_ and type the `CREATE TABLE` statements directly at first‚Äîin fact, when just starting out, you might prefer to go this route first because it may take a few iterations to get your DDL exactly right.

### `DROP TABLE`, `ALTER TABLE`, and `\d`
Some useful new options while you‚Äôre in this phase include:
* The [SQL `DROP TABLE` statement](https://www.postgresql.org/docs/current/sql-droptable.html) removes a table from the database in case you need to create it again
* [`ALTER TABLE` is an SQL alternative](https://www.postgresql.org/docs/current/sql-altertable.html) that allows you to just change an existing table rather than removing it and creating it over‚Äîimportant if your table already has data in it! But if you do this on the fly, make sure to still capture the latest changes in your _schema.sql_ so that if you decide to start over, you‚Äôll get the latest version of your table definition
* The _psql_ `\dt` command will show you the list of current tables in your database
* Also available in _psql_ is `\d <table name>`, which will show you the definition of a given table

Don‚Äôt forget that you can use `\h` in _psql_ to get online help on any SQL statement and `\?` will provide help on the non-SQL utility commands.

## Put the ‚ÄúData‚Äù in ‚ÄúDatabase‚Äù
Technically, once your tables are defined your database is ready to go. But‚Ä¶it‚Äôs empty. You can use it but you can‚Äôt do much with it until you give it some data. This is where your dataset‚Äôs files come in‚Äîit‚Äôs time to get their information into those tables.

There are many ways to get data into a relational database in general and PostgreSQL in particular. For this case study, we will take a highly portable approach to doing this: feeding `INSERT` statements into _psql_. It‚Äôs highly portable because the approach is independent of any particular programming language, utility, or library‚Äîit doesn‚Äôt matter how you generate the `INSERT` statements; it just matters that you _do_ generate them.

That said, this may not be the most efficient way to get data in there, but we‚Äôve got to start someplace right?

The [_movie_loader.py_](./movie_loader.py) and [_rating_loader.py_](./rating_loader.py) programs will read the _movie_titles.csv_ and _combined_data*.txt_ files, respectively, and print `INSERT` statements corresponding to each movie and rating record in those files. You‚Äôll notice that _rating_loader.py_ uses similar logic to our previous preprocessor program by watching out for those movie ID lines (the ones with just a movie ID followed by a colon) and making sure that the current movie ID is included in each emitted `INSERT` statement.

When run by themselves, the `INSERT` statements are simply printed out. Once you are satisfied that the statements are indeed correct‚Äîi.e., they will correctly add the data from the files into the database‚Äîyou can then use the handy `|` directive to send those printed commands into _psql_:

    python3 movie_loader.py | psql postgres://localhost/postgres
    python3 rating_loader.py | psql postgres://localhost/postgres

Alternatively, you can write the output of those programs into ‚ÄúSQL script‚Äù files then make _psql_ execute those files:

    python3 movie_loader.py > movie_data.sql
    psql postgres://localhost/postgres -f movie_data.sql
    python3 rating_loader.py > rating_data.sql
    psql postgres://localhost/postgres -f rating_data.sql

### `DELETE`
As with schema definition, you might need to iterate through this when you‚Äôre writing your own loaders, potentially resulting in leftover data that would get duplicated if you ran your loader again. To assist with this, you may either use `DROP TABLE` to remove a table entirely‚Äîmeaning you have to invoke `CREATE TABLE` again‚Äîor you can instead use the `DELETE` statement. The super-concise (and super-dangerous!) `DELETE FROM <table>;` will unconditionally remove every row in that table. Use it with caution! To be more precise about what to delete, you can add a `WHERE` clause to the `DELETE` statement: it works just like the `WHERE` clause in `SELECT`, except that matching rows are _removed_ from the table rather than returned as a result.

### A Note About Scale
The Netflix Prize dataset consists of around 17,700 movies‚Äîfairly small as real datasets go. But note there are _more than **100,000,000** ratings_ in the dataset taking up more than 2 gigabytes of data‚Äîthat‚Äôs the real deal! This scale means that:
* Converting to `INSERT` statements means you‚Äôll have more than 100 million such statements‚Äîpre-writing these commands out to a file will produce a very large file!
* Executing 100 million `INSERT` statements into _psql_ will definitely take some time‚Äîon the order of _hours_ on current hardware

Thus, when launching into this step of the case study, do make sure of the following:
* Your computer has enough storage space for 100 million ratings records
* You start the ratings load at a time when you can leave your computer running for many hours‚Äîovernight would be ideal

We definitely aren‚Äôt fooling around here üßê ‚Äîbut the hope is that the scale is worth it, because it will help you truly appreciate what generalized database management systems do for us. Imagine writing programs that do a sequential scan of these ratings _every single time_ you want to retrieve something from this dataset. Or better yet, just don‚Äôt imagine it and skip right to using a database üòÇ

In case we haven‚Äôt made it clear enough yet: it will take **multiple hours** and **multiple gigabytes** to fully load PostgreSQL with the full ratings data. So plan accordingly.

The good news is that, while this process may be running on one terminal window, you can always connect to the database concurrently on another window. So you can definitely start issuing queries and practicing things even while database loading is going on‚Äîyou merely won‚Äôt have all of the data at your fingertips yet until all of the records have landed.

## Time for Some Play<i>SQ</i>oo<i>L</i>
Once the database has some data in it (doesn‚Äôt have to be completely full yet, as mentioned previously), you can start honing your SQL skills. You can start with the level of SQL shown in the [Super Basic SQL crib sheet](http://dondi.lmu.build/share/db/super-basic-sql.pdf) and the original [Netflix DB Fiddle example](https://www.db-fiddle.com/f/o2ohcGVAgHZQg4teg1s9jW/6) (minus references to the _viewer_ table)‚Äîjust think of a question in plain English and see if you can translate that into a corresponding [`SELECT` statement](https://www.postgresql.org/docs/current/sql-select.html). The PostgreSQL documentation also provides a [querying tutorial](https://www.postgresql.org/docs/current/tutorial-select.html) if you want to see additional examples at a basic level.

Issuing `SELECT` statements is a matter of running _psql_ on your database server then typing the statement directly at the _psql_ prompt. Make sure to terminate your statements with a semicolon `;` so that _psql_ knows when you‚Äôre done.

* List all columns for movies containing both `'and'` and `'of'` in their titles, sorted ascending by year then title:
```
postgres=# SELECT * FROM movie WHERE title LIKE '% and %' AND title LIKE '% of %' ORDER BY year, title;
  id   | year |                                title                                 
-------+------+----------------------------------------------------------------------
  3790 | 1921 | Avant-Garde: Experimental Cinema of the 1920s and '30s
  2449 | 1939 | The Private Lives of Elizabeth and Essex
 12416 | 1942 | Sherlock Holmes and the Voice of Terror
  7220 | 1946 | Anna and the King of Siam
 17715 | 1947 | The Adventures of Ma and Pa Kettle: Vol. 1
   ...
   ...
(96 rows)
```

* List movie titles released in the 20th century with the substring `'future'` in their titles, case-insensitively, sorted ascending by title:
```
postgres=# SELECT title FROM movie WHERE year < 2001 AND title ILIKE '%future%' ORDER BY title;
                title                 
--------------------------------------
 Back to the Future
 Back to the Future Part II
 Back to the Future Part III
 Future War
 Futuresport
 Ivan Vasilievich: Back to the Future
 Megaman: Battle for the Future
 The X-Files: Fight the Future
(8 rows)
```

Due to the size of the _rating_ table, be prepared for some of these queries to take a few moments (but definitely not hours):

* Determine the average rating given by all viewers on the month of October, 2004:
```
postgres=# SELECT AVG(rating) FROM rating WHERE date_rated >= '2004-10-01' AND date_rated <= '2004-10-31';
        avg         
--------------------
 3.6811938432580795
(1 row)
```

* List the titles of movies that got a rating of 1 on December 25, 2002, sorted ascending by title:
```
postgres=# SELECT DISTINCT title FROM movie, rating WHERE movie.id = rating.movie_id AND rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
                                    title                                    
-----------------------------------------------------------------------------
 102 Dalmatians
 13 Conversations About One Thing
 3000 Miles to Graceland
 A Couch in New York
 A Fish Called Wanda
   ...
   ...
(429 rows)
```

* List the year and rating count for movies released before 1910 or after 2000, sorted descending by year:
```
postgres=# SELECT year, count(*) FROM movie, rating WHERE movie.id = rating.movie_id AND (year < 1910 OR year > 2000) GROUP BY year ORDER BY year DESC;
 year |  count   
------+----------
 2005 |  1983802
 2004 | 10456339
 2003 |  9576604
 2002 |  8640932
 2001 |  7241888
 1909 |      109
 1896 |      152
(7 rows)
```

The possibilities go on and on‚Ä¶the fun never stops!

### A Word About Correctness
When you‚Äôre still starting out with SQL and you have this much data, it will be good to know whether the query you formulated really did return the results you were looking for. For some queries, this will be self-evident‚Äîyou can check from the movie titles, years, or other attributes whether the returned records meet your specified conditions. However, some queries are a little harder to check. For example, with the query given at the end of the previous section, how would you know whether the counts were correct?

Although the very large results may be harder to tally, for the smaller counts, one can potentially formulate a different but related query then see whether those results align with the original query‚Äôs results. Thus, one can potentially retrieve all ratings given to movies released in 1896 to see whether there are indeed 152 of them:

```
postgres=# SELECT * FROM movie, rating WHERE movie.id = rating.movie_id AND year = 1896;
  id  | year |             title             | movie_id | viewer_id | rating | date_rated 
------+------+-------------------------------+----------+-----------+--------+------------
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |   2482518 |      5 | 2004-09-24
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |    440294 |      5 | 2004-03-05
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |   1062414 |      4 | 2004-06-03
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |   2092745 |      5 | 2005-04-07
   ...
   ...
(152 rows)
```

After paging through all of the results, one does get the final count message of `(152 rows)` so this helps to assure us that our earlier, more complex query did indeed give us the result that we wanted.

### Learning/Using Other Comparators
Take note that PostgreSQL is capable of a wide range of comparators for strings and other data types‚Äîsome are part of the universal SQL standard and some are specific to PostgreSQL‚Äôs _dialect_ of SQL. It isn‚Äôt necessary to know each of these variations off the top of your head but it‚Äôs good to know that they‚Äôre at there and also good to know how to look them up when needed:
- [Standard comparators](https://www.postgresql.org/docs/current/functions-comparison.html)
- [String pattern matching](https://www.postgresql.org/docs/current/functions-matching.html)
- [Date/time comparators](https://www.postgresql.org/docs/current/functions-datetime.html)

The full range of capabilities can be found in the [Functions and Operators](https://www.postgresql.org/docs/current/functions.html) section of the PostgreSQL documentation. It‚Äôs actually quite mind-boggling‚Äîjust take a peek and walk away with what seems immediately useful or interesting. Plan on looking things up later as needed.

### Using `LIMIT`
For some information needs, you don‚Äôt need to know _all_ of the results at once‚Äîyou might want just a subset. That‚Äôs where `LIMIT` comes in. Adding a `LIMIT` to a `SELECT` statement, followed by some number _n_, will only return a maximum of _n_ matching results. When accompanied with the correct `ORDER BY` clause, this allows you to issue top-_n_ queries. In the broader context of web services, `LIMIT` facilitates _pagination_, where users only see a maximum number of records at a time and need to _page_ forward or backward through the results in distinct steps.

* List the IDs and average ratings of the top 5 movies in terms of average rating, sorted descending by average rating then by movie ID in case of a tie:
```
postgres=# SELECT movie_id, AVG(rating) FROM rating GROUP BY movie_id ORDER BY AVG(rating) DESC, movie_id LIMIT 5;
 movie_id |        avg         
----------+--------------------
    14961 | 4.7232699256835072
     7230 | 4.7166108250932963
     7057 | 4.7026110636480137
     3456 | 4.6709891019450959
     9864 | 4.6388093875214654
(5 rows)
```

For full details, [this section](https://www.postgresql.org/docs/current/queries-limit.html) of the PostgreSQL documentation describes the `LIMIT` keyword (along with `OFFSET`, which we won‚Äôt address here but should be easy to pick up upon reading that page).

### More Explicit Relationships with `INNER JOIN`
You have seen so far that we can connect multiple tables by naming them in the `FROM` clause of a `SELECT` statement then specifying a condition in the `WHERE` clause that determines how rows from one table are ‚Äúmatched up‚Äù with another:

```
SELECT DISTINCT title FROM movie, rating WHERE movie.id = rating.movie_id AND rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
```

This works perfectly well but there is an alternate form for specifying this matchup which more explicitly states that a relationship exists between two tables: this form is the `INNER JOIN` expression. Using `INNER JOIN` places the related tables and the condition that connects them closer together, all in the `FROM` clause. This makes the intent of the query somewhat clearer and opens up the possibility for other kinds of `JOIN`s which we will see later:

```
SELECT DISTINCT title FROM movie INNER JOIN rating ON movie.id = rating.movie_id WHERE rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
```

When using the `INNER JOIN` expression, the two tables are named before and after `INNER JOIN` (thus making `INNER JOIN` feel like a binary operation, which it is) and an `ON` clause states the condition that connects the records of one table to the other. This moves the condition outside of the `WHERE` clause, thus more cleanly separating which conditions establish relationships vs. which conditions are meant to narrow down the data being requested.

Refer to [this tutorial](https://www.postgresql.org/docs/current/tutorial-join.html) for additional discussion of `JOIN`. A more comprehensive description of `JOIN` is provided in the [reference page for the `SELECT` statement](https://www.postgresql.org/docs/current/sql-select.html).

### The `HAVING` Clause: `WHERE` for `GROUP BY`
As you practice using SQL, you will eventually notice that there‚Äôs something that the `WHERE` clause appears to unintuitively reject: conditions on aggregate functions like `COUNT`, `AVG`, `MAX`, and `MIN`. For example, this query, which tries to determine movie IDs with a low average rating, generates an error:

```
postgres=# SELECT movie_id, AVG(rating) FROM rating WHERE AVG(rating) < 2 GROUP BY movie_id;
ERROR:  aggregate functions are not allowed in WHERE
LINE 1: SELECT movie_id, AVG(rating) FROM rating WHERE AVG(rating) <...
```

Conditions that involve these aggregate functions are disallowed from `WHERE` because `WHERE` is supposed to operate on _individual rows_ and not _groups of rows_, which is what aggregate functions use. Instead, a _separate_ clause called `HAVING` is used for conditions that involve these groups, and `HAVING` appears after `GROUP BY` to drive this point home:

```
postgres=# SELECT movie_id, AVG(rating) FROM rating GROUP BY movie_id HAVING AVG(rating) < 2;
 movie_id |        avg         
----------+--------------------
       41 | 1.6344086021505376
       53 | 1.6754385964912281
       86 | 1.9545454545454545
      151 | 1.9545454545454545
      415 | 1.7619047619047619
   ...
   ...
(313 rows)
```

One way to remember whether a condition goes in `WHERE` vs. `HAVING` is to ask yourself: ‚Äúif I‚Äôm looking at a _single row_ from one or more tables, do I have enough information to answer whether a condition is fulfilled?‚Äù If the answer is ‚Äúyes,‚Äù then you are looking at a `WHERE` condition. If the answer is ‚Äúno‚Äù‚Äîfor example, you can‚Äôt tell if a single row will fulfill some `COUNT` or `AVG` condition because you need multiple rows to calculate those in the first place‚Äîthen that condition belongs in `HAVING`.

### Making PostgreSQL `EXPLAIN` Itself
By way of a preview to future discussions, one last fun thing you can do with _psql_ is the `EXPLAIN` statement. `EXPLAIN` is simple: if you prepend `EXPLAIN` to any `SELECT` statement, instead of performing the query PostgreSQL will show _how_ it will perform the query. How many nested loops will there be? What operations will be brute-force scan vs. a different kind of algorithm? How much data will be covered. The availability of `EXPLAIN` is a great way to take a peek into the inner workings of the database system:

```
postgres=# EXPLAIN SELECT DISTINCT title FROM movie, rating WHERE movie.id = rating.movie_id AND rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
                                          QUERY PLAN                                           
-----------------------------------------------------------------------------------------------
 Unique  (cost=1174410.13..1174420.18 rows=2011 width=22)
   ->  Sort  (cost=1174410.13..1174415.15 rows=2011 width=22)
         Sort Key: movie.title
         ->  Nested Loop  (cost=1000.29..1174299.79 rows=2011 width=22)
               ->  Gather  (cost=1000.00..1172342.43 rows=2011 width=4)
                     Workers Planned: 2
                     ->  Parallel Seq Scan on rating  (cost=0.00..1171141.33 rows=838 width=4)
                           Filter: ((rating = 1) AND (date_rated = '2002-12-25'::date))
               ->  Index Scan using movie_pkey on movie  (cost=0.29..0.97 rows=1 width=26)
                     Index Cond: (id = rating.movie_id)
(10 rows)
```

## Hello DAL-ly
Work in progress‚Äîstay tuned!
